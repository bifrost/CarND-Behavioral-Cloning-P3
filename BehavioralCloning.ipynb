{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/center_2018_08...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/right_2018_08_...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/center_2018_08...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/right_2018_08_...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/center_2018_08...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/right_2018_08_...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/center_2018_08...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/right_2018_08_...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/center_2018_08...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...</td>\n",
       "      <td>/Users/dan/Desktop/car-sim1/IMG/right_2018_08_...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  /Users/dan/Desktop/car-sim1/IMG/center_2018_08...   \n",
       "1  /Users/dan/Desktop/car-sim1/IMG/center_2018_08...   \n",
       "2  /Users/dan/Desktop/car-sim1/IMG/center_2018_08...   \n",
       "3  /Users/dan/Desktop/car-sim1/IMG/center_2018_08...   \n",
       "4  /Users/dan/Desktop/car-sim1/IMG/center_2018_08...   \n",
       "\n",
       "                                                   1  \\\n",
       "0  /Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...   \n",
       "1  /Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...   \n",
       "2  /Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...   \n",
       "3  /Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...   \n",
       "4  /Users/dan/Desktop/car-sim1/IMG/left_2018_08_2...   \n",
       "\n",
       "                                                   2    3    4  5         6  \n",
       "0  /Users/dan/Desktop/car-sim1/IMG/right_2018_08_...  0.0  0.0  0  0.000006  \n",
       "1  /Users/dan/Desktop/car-sim1/IMG/right_2018_08_...  0.0  0.0  0  0.000012  \n",
       "2  /Users/dan/Desktop/car-sim1/IMG/right_2018_08_...  0.0  0.0  0  0.000006  \n",
       "3  /Users/dan/Desktop/car-sim1/IMG/right_2018_08_...  0.0  0.0  0  0.000004  \n",
       "4  /Users/dan/Desktop/car-sim1/IMG/right_2018_08_...  0.0  0.0  0  0.000010  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = './data/car-sim1/'\n",
    "img_path = './data/car-sim1/IMG/'\n",
    "df = pd.read_csv(data_path+'driving_log.csv', header=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "lines = []\n",
    "with open(data_path+'driving_log.csv') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    #next(reader, None)\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "        \n",
    "def process_image(img): \n",
    "    # preprocess input\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (6861, 160, 320, 3)\n",
      "Y_train: (6861,)\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "measurements = []\n",
    "for line in lines:\n",
    "    '''\n",
    "    source_path = line[0]\n",
    "    filename = source_path.split('/')[-1]\n",
    "    current_path = img_path + filename\n",
    "    image = cv2.imread(current_path)\n",
    "    images.append(image)\n",
    "    measurement = float(line[3])\n",
    "    measurements.append(measurement)\n",
    "    '''\n",
    "    \n",
    "    steering_center = float(line[3])\n",
    "\n",
    "    # create adjusted steering measurements for the side camera images\n",
    "    correction = 0.2 # this is a parameter to tune\n",
    "    steering_left = steering_center + correction\n",
    "    steering_right = steering_center - correction\n",
    "\n",
    "    # read in images from center, left and right cameras\n",
    "    filename = line[0].split('/')[-1]\n",
    "    img_center = process_image(cv2.imread(img_path + filename))\n",
    "    filename = line[1].split('/')[-1]\n",
    "    img_left = process_image(cv2.imread(img_path + filename))\n",
    "    filename = line[2].split('/')[-1]\n",
    "    img_right = process_image(cv2.imread(img_path + filename))\n",
    "\n",
    "    # add images and angles to data set\n",
    "    images.extend([img_center, img_left, img_right])\n",
    "    measurements.extend([steering_center, steering_left, steering_right])\n",
    "    \n",
    "X_train = np.array(images)\n",
    "Y_train = np.array(measurements)\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('Y_train:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: (2287, 160, 320, 3)\n",
      "measurements: (2287,)\n"
     ]
    }
   ],
   "source": [
    "images_center = []\n",
    "images_left = []\n",
    "images_right = []\n",
    "measurements_center = []\n",
    "measurements_left = []\n",
    "measurements_right = []\n",
    "for line in lines:\n",
    "\n",
    "    steering_center = float(line[3])\n",
    "\n",
    "    # create adjusted steering measurements for the side camera images\n",
    "    correction = 0.25 # this is a parameter to tune\n",
    "    steering_left = steering_center + correction\n",
    "    steering_right = steering_center - correction\n",
    "\n",
    "    # read in images from center, left and right cameras\n",
    "    filename = line[0].split('/')[-1]\n",
    "    img_center = process_image(cv2.imread(img_path + filename))\n",
    "    \n",
    "    filename = line[1].split('/')[-1]\n",
    "    img_left = process_image(cv2.imread(img_path + filename))\n",
    "    \n",
    "    filename = line[2].split('/')[-1]\n",
    "    img_right = process_image(cv2.imread(img_path + filename))\n",
    "    \n",
    "    # add images and angles to data set\n",
    "    images_center.append(img_center)\n",
    "    images_left.append(img_left)\n",
    "    images_right.append(img_right)\n",
    "    \n",
    "    measurements_center.append(steering_center)\n",
    "    measurements_left.append(steering_left)\n",
    "    measurements_right.append(steering_right)\n",
    "    \n",
    "# convert to array\n",
    "images_center = np.array(images_center)\n",
    "images_left = np.array(images_left)\n",
    "images_right = np.array(images_right)\n",
    "\n",
    "measurements_center = np.array(measurements_center)\n",
    "measurements_left = np.array(measurements_left)\n",
    "measurements_right = np.array(measurements_right)\n",
    "    \n",
    "print('images:', images_center.shape)\n",
    "print('measurements:', measurements_center.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data and augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_idx: 1600\n",
      "val_idx: 343\n",
      "test_idx: 344\n",
      "images_train: (12348, 160, 320, 3)\n",
      "measurements_train: (12348,)\n",
      "images_val: (343, 160, 320, 3)\n",
      "measurements_val: (343,)\n",
      "images_test: (344, 160, 320, 3)\n",
      "measurements_test: (344,)\n"
     ]
    }
   ],
   "source": [
    "# split 70/15/15\n",
    "length = len(measurements_center)\n",
    "train_size = int(length * 0.7)\n",
    "\n",
    "indices = np.random.permutation(length)\n",
    "train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "\n",
    "val_size = int(len(test_idx) * 0.5)\n",
    "val_idx, test_idx = test_idx[:val_size], test_idx[val_size:]\n",
    "\n",
    "print('train_idx:', len(train_idx))\n",
    "print('val_idx:', len(val_idx))\n",
    "print('test_idx:', len(test_idx))\n",
    "\n",
    "images_train = np.concatenate((\n",
    "    images_center[train_idx],\n",
    "    images_left[train_idx],\n",
    "    images_right[train_idx],\n",
    "    images_left[val_idx],\n",
    "    images_right[val_idx],\n",
    "    images_left[test_idx], \n",
    "    images_right[test_idx]\n",
    "))\n",
    "measurements_train = np.concatenate((\n",
    "    measurements_center[train_idx], \n",
    "    measurements_left[train_idx], \n",
    "    measurements_right[train_idx], \n",
    "    measurements_left[val_idx], \n",
    "    measurements_right[val_idx], \n",
    "    measurements_left[test_idx], \n",
    "    measurements_right[test_idx]\n",
    "))\n",
    "\n",
    "images_val, measurements_val = images_center[val_idx], measurements_center[val_idx]\n",
    "images_test, measurements_test = images_center[test_idx], measurements_center[test_idx]\n",
    "\n",
    "# augmented train set\n",
    "augmented_images = []\n",
    "augmented_measurements = []\n",
    "for image, measurement in zip(images_train, measurements_train):\n",
    "    augmented_images.append(image)\n",
    "    augmented_measurements.append(measurement)\n",
    "    augmented_images.append(cv2.flip(image,1))\n",
    "    augmented_measurements.append(measurement*-1.0)\n",
    "    \n",
    "images_train = np.array(augmented_images)\n",
    "measurements_train = np.array(augmented_measurements)\n",
    "    \n",
    "    \n",
    "print('images_train:', images_train.shape)\n",
    "print('measurements_train:', measurements_train.shape)\n",
    "print('images_val:', images_val.shape)\n",
    "print('measurements_val:', measurements_val.shape)\n",
    "print('images_test:', images_test.shape)\n",
    "print('measurements_test:', measurements_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (9600, 160, 320, 3)\n",
      "Y_train: (9600,)\n"
     ]
    }
   ],
   "source": [
    "X_train = images_train\n",
    "Y_train = measurements_train\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('Y_train:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10972 samples, validate on 2744 samples\n",
      "Epoch 1/2\n",
      "10972/10972 [==============================] - 7s 677us/step - loss: 0.0980 - val_loss: 0.1220\n",
      "Epoch 2/2\n",
      "10972/10972 [==============================] - 7s 623us/step - loss: 0.0578 - val_loss: 0.1509\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Flatten(input_shape=(160, 320, 3)))\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.0001))\n",
    "model.fit(X_train, Y_train, validation_split=0.2, shuffle=True, epochs=2)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10977 samples, validate on 2745 samples\n",
      "Epoch 1/3\n",
      "10977/10977 [==============================] - 10s 889us/step - loss: 0.5664 - val_loss: 0.0745\n",
      "Epoch 2/3\n",
      "10977/10977 [==============================] - 9s 796us/step - loss: 0.0437 - val_loss: 0.0645\n",
      "Epoch 3/3\n",
      "10977/10977 [==============================] - 9s 803us/step - loss: 0.0374 - val_loss: 0.0639\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(160,320,3)))\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5))\n",
    "model.add(Conv2D(6, (5,5), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(6, (5,5), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Dense(84))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "model.fit(X_train, Y_train, validation_split=0.2, shuffle=True, epochs=3)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "modelTransfer = VGG16(weights='imagenet', include_top=False, pooling='max')\n",
    "\n",
    "#x = modelTransfer.output\n",
    "#x = GlobalMaxPooling2D()(x)\n",
    "#net = Model(inputs=modelTransfer.input, outputs=x)\n",
    "modelTransfer.summary()\n",
    "\n",
    "def get_bottleneck_feature(img):\n",
    "    # convert 3D tensor to 4D tensor with shape (1, ?, ?, 3) and return 4D tensor\n",
    "    x = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    # preprocess input\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    return modelTransfer.predict(x)\n",
    "\n",
    "def get_bottleneck_features(images):\n",
    "    bottleneck_features = []\n",
    "\n",
    "    for img in tqdm(images):\n",
    "        bottleneck_features.append(get_bottleneck_feature(img))\n",
    "        #try:\n",
    "        #    bottleneck_features.append(bottleneck_feature(img))\n",
    "        #except OSError:\n",
    "        #    None \n",
    "            \n",
    "    return np.vstack(bottleneck_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9600/9600 [00:58<00:00, 164.63it/s]\n",
      "100%|██████████| 343/343 [00:02<00:00, 162.82it/s]\n",
      "100%|██████████| 344/344 [00:02<00:00, 154.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (9600, 512)\n",
      "X_val: (343, 512)\n",
      "X_test: (344, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "X_train = get_bottleneck_features(images_train)\n",
    "Y_train = measurements_train\n",
    "\n",
    "X_val = get_bottleneck_features(images_val)\n",
    "Y_val = measurements_val\n",
    "\n",
    "X_test = get_bottleneck_features(images_test)\n",
    "Y_test = measurements_test\n",
    "\n",
    "bottleneck_feature = {\n",
    "    'X_train': X_train,\n",
    "    'Y_train': Y_train,\n",
    "    'X_val': X_val,\n",
    "    'Y_val': Y_val,\n",
    "    'X_test': X_test,\n",
    "    'Y_test': Y_test\n",
    "}\n",
    "\n",
    "with open('models/bottleneck_feature.pickle', 'wb') as f:\n",
    "    pickle.dump(bottleneck_feature, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('X_val:', X_val.shape)\n",
    "print('X_test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 512)\n",
      "(9600,)\n",
      "(343, 512)\n",
      "(343,)\n",
      "(344, 512)\n",
      "(344,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/bottleneck_feature.pickle', 'rb') as f:\n",
    "    bottleneck_feature = pickle.load(f)\n",
    "    \n",
    "X_train = bottleneck_feature['X_train']\n",
    "Y_train = bottleneck_feature['Y_train']\n",
    "\n",
    "X_val = bottleneck_feature['X_val']\n",
    "Y_val = bottleneck_feature['Y_val']\n",
    "\n",
    "X_test = bottleneck_feature['X_test']\n",
    "Y_test = bottleneck_feature['Y_test']\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 197,377\n",
      "Trainable params: 197,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.02), input_shape=X_train.shape[1:]))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.02)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9600 samples, validate on 343 samples\n",
      "Epoch 1/100\n",
      "9600/9600 [==============================] - 1s 140us/step - loss: 68.0117 - val_loss: 30.8114\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 30.81140, saving model to models/weights.best.hdf5\n",
      "Epoch 2/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 23.0870 - val_loss: 20.5189\n",
      "\n",
      "Epoch 00002: val_loss improved from 30.81140 to 20.51890, saving model to models/weights.best.hdf5\n",
      "Epoch 3/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 17.9834 - val_loss: 18.1462\n",
      "\n",
      "Epoch 00003: val_loss improved from 20.51890 to 18.14622, saving model to models/weights.best.hdf5\n",
      "Epoch 4/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 15.2971 - val_loss: 15.3619\n",
      "\n",
      "Epoch 00004: val_loss improved from 18.14622 to 15.36188, saving model to models/weights.best.hdf5\n",
      "Epoch 5/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 13.8874 - val_loss: 14.3798\n",
      "\n",
      "Epoch 00005: val_loss improved from 15.36188 to 14.37976, saving model to models/weights.best.hdf5\n",
      "Epoch 6/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 12.9079 - val_loss: 13.7186\n",
      "\n",
      "Epoch 00006: val_loss improved from 14.37976 to 13.71865, saving model to models/weights.best.hdf5\n",
      "Epoch 7/100\n",
      "9600/9600 [==============================] - 1s 71us/step - loss: 12.2444 - val_loss: 12.5962\n",
      "\n",
      "Epoch 00007: val_loss improved from 13.71865 to 12.59616, saving model to models/weights.best.hdf5\n",
      "Epoch 8/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 11.6650 - val_loss: 12.0349\n",
      "\n",
      "Epoch 00008: val_loss improved from 12.59616 to 12.03490, saving model to models/weights.best.hdf5\n",
      "Epoch 9/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 11.2412 - val_loss: 11.6819\n",
      "\n",
      "Epoch 00009: val_loss improved from 12.03490 to 11.68185, saving model to models/weights.best.hdf5\n",
      "Epoch 10/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 10.8723 - val_loss: 10.9343\n",
      "\n",
      "Epoch 00010: val_loss improved from 11.68185 to 10.93431, saving model to models/weights.best.hdf5\n",
      "Epoch 11/100\n",
      "9600/9600 [==============================] - 1s 78us/step - loss: 10.4865 - val_loss: 10.7487\n",
      "\n",
      "Epoch 00011: val_loss improved from 10.93431 to 10.74865, saving model to models/weights.best.hdf5\n",
      "Epoch 12/100\n",
      "9600/9600 [==============================] - 1s 81us/step - loss: 10.1785 - val_loss: 10.0913\n",
      "\n",
      "Epoch 00012: val_loss improved from 10.74865 to 10.09133, saving model to models/weights.best.hdf5\n",
      "Epoch 13/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 9.8437 - val_loss: 9.7318\n",
      "\n",
      "Epoch 00013: val_loss improved from 10.09133 to 9.73175, saving model to models/weights.best.hdf5\n",
      "Epoch 14/100\n",
      "9600/9600 [==============================] - 1s 78us/step - loss: 9.5567 - val_loss: 9.4579\n",
      "\n",
      "Epoch 00014: val_loss improved from 9.73175 to 9.45791, saving model to models/weights.best.hdf5\n",
      "Epoch 15/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 9.3002 - val_loss: 9.1612\n",
      "\n",
      "Epoch 00015: val_loss improved from 9.45791 to 9.16124, saving model to models/weights.best.hdf5\n",
      "Epoch 16/100\n",
      "9600/9600 [==============================] - 1s 82us/step - loss: 9.0529 - val_loss: 8.9269\n",
      "\n",
      "Epoch 00016: val_loss improved from 9.16124 to 8.92695, saving model to models/weights.best.hdf5\n",
      "Epoch 17/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 8.8210 - val_loss: 8.7296\n",
      "\n",
      "Epoch 00017: val_loss improved from 8.92695 to 8.72963, saving model to models/weights.best.hdf5\n",
      "Epoch 18/100\n",
      "9600/9600 [==============================] - 1s 79us/step - loss: 8.5938 - val_loss: 8.5502\n",
      "\n",
      "Epoch 00018: val_loss improved from 8.72963 to 8.55024, saving model to models/weights.best.hdf5\n",
      "Epoch 19/100\n",
      "9600/9600 [==============================] - 1s 79us/step - loss: 8.3701 - val_loss: 8.2434\n",
      "\n",
      "Epoch 00019: val_loss improved from 8.55024 to 8.24345, saving model to models/weights.best.hdf5\n",
      "Epoch 20/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 8.1384 - val_loss: 7.9814\n",
      "\n",
      "Epoch 00020: val_loss improved from 8.24345 to 7.98137, saving model to models/weights.best.hdf5\n",
      "Epoch 21/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 7.8930 - val_loss: 7.7844\n",
      "\n",
      "Epoch 00021: val_loss improved from 7.98137 to 7.78439, saving model to models/weights.best.hdf5\n",
      "Epoch 22/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 7.6610 - val_loss: 7.4971\n",
      "\n",
      "Epoch 00022: val_loss improved from 7.78439 to 7.49705, saving model to models/weights.best.hdf5\n",
      "Epoch 23/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 7.3662 - val_loss: 7.1929\n",
      "\n",
      "Epoch 00023: val_loss improved from 7.49705 to 7.19291, saving model to models/weights.best.hdf5\n",
      "Epoch 24/100\n",
      "9600/9600 [==============================] - 1s 79us/step - loss: 7.0800 - val_loss: 7.0772\n",
      "\n",
      "Epoch 00024: val_loss improved from 7.19291 to 7.07720, saving model to models/weights.best.hdf5\n",
      "Epoch 25/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 6.7972 - val_loss: 6.6218\n",
      "\n",
      "Epoch 00025: val_loss improved from 7.07720 to 6.62181, saving model to models/weights.best.hdf5\n",
      "Epoch 26/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 6.5038 - val_loss: 6.3401\n",
      "\n",
      "Epoch 00026: val_loss improved from 6.62181 to 6.34015, saving model to models/weights.best.hdf5\n",
      "Epoch 27/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 6.1998 - val_loss: 6.0825\n",
      "\n",
      "Epoch 00027: val_loss improved from 6.34015 to 6.08245, saving model to models/weights.best.hdf5\n",
      "Epoch 28/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 5.9016 - val_loss: 5.7518\n",
      "\n",
      "Epoch 00028: val_loss improved from 6.08245 to 5.75183, saving model to models/weights.best.hdf5\n",
      "Epoch 29/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 5.5966 - val_loss: 5.4304\n",
      "\n",
      "Epoch 00029: val_loss improved from 5.75183 to 5.43041, saving model to models/weights.best.hdf5\n",
      "Epoch 30/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 5.2729 - val_loss: 5.1219\n",
      "\n",
      "Epoch 00030: val_loss improved from 5.43041 to 5.12187, saving model to models/weights.best.hdf5\n",
      "Epoch 31/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 4.9702 - val_loss: 4.8439\n",
      "\n",
      "Epoch 00031: val_loss improved from 5.12187 to 4.84388, saving model to models/weights.best.hdf5\n",
      "Epoch 32/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 4.6515 - val_loss: 4.4864\n",
      "\n",
      "Epoch 00032: val_loss improved from 4.84388 to 4.48639, saving model to models/weights.best.hdf5\n",
      "Epoch 33/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 4.3346 - val_loss: 4.1744\n",
      "\n",
      "Epoch 00033: val_loss improved from 4.48639 to 4.17438, saving model to models/weights.best.hdf5\n",
      "Epoch 34/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 4.0422 - val_loss: 3.9148\n",
      "\n",
      "Epoch 00034: val_loss improved from 4.17438 to 3.91477, saving model to models/weights.best.hdf5\n",
      "Epoch 35/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 3.7490 - val_loss: 3.5884\n",
      "\n",
      "Epoch 00035: val_loss improved from 3.91477 to 3.58844, saving model to models/weights.best.hdf5\n",
      "Epoch 36/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 3.4560 - val_loss: 3.3071\n",
      "\n",
      "Epoch 00036: val_loss improved from 3.58844 to 3.30708, saving model to models/weights.best.hdf5\n",
      "Epoch 37/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 3.1854 - val_loss: 3.0876\n",
      "\n",
      "Epoch 00037: val_loss improved from 3.30708 to 3.08762, saving model to models/weights.best.hdf5\n",
      "Epoch 38/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 2.9222 - val_loss: 2.7881\n",
      "\n",
      "Epoch 00038: val_loss improved from 3.08762 to 2.78814, saving model to models/weights.best.hdf5\n",
      "Epoch 39/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 2.6662 - val_loss: 2.5421\n",
      "\n",
      "Epoch 00039: val_loss improved from 2.78814 to 2.54215, saving model to models/weights.best.hdf5\n",
      "Epoch 40/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 2.4335 - val_loss: 2.3190\n",
      "\n",
      "Epoch 00040: val_loss improved from 2.54215 to 2.31903, saving model to models/weights.best.hdf5\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600/9600 [==============================] - 1s 73us/step - loss: 2.2174 - val_loss: 2.1081\n",
      "\n",
      "Epoch 00041: val_loss improved from 2.31903 to 2.10811, saving model to models/weights.best.hdf5\n",
      "Epoch 42/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 2.0163 - val_loss: 1.9206\n",
      "\n",
      "Epoch 00042: val_loss improved from 2.10811 to 1.92058, saving model to models/weights.best.hdf5\n",
      "Epoch 43/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 1.8370 - val_loss: 1.7780\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.92058 to 1.77797, saving model to models/weights.best.hdf5\n",
      "Epoch 44/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 1.6680 - val_loss: 1.6094\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.77797 to 1.60936, saving model to models/weights.best.hdf5\n",
      "Epoch 45/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 1.5159 - val_loss: 1.4516\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.60936 to 1.45159, saving model to models/weights.best.hdf5\n",
      "Epoch 46/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 1.3801 - val_loss: 1.3341\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.45159 to 1.33411, saving model to models/weights.best.hdf5\n",
      "Epoch 47/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 1.2622 - val_loss: 1.2053\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.33411 to 1.20525, saving model to models/weights.best.hdf5\n",
      "Epoch 48/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 1.1519 - val_loss: 1.1116\n",
      "\n",
      "Epoch 00048: val_loss improved from 1.20525 to 1.11162, saving model to models/weights.best.hdf5\n",
      "Epoch 49/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 1.0602 - val_loss: 1.0181\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.11162 to 1.01807, saving model to models/weights.best.hdf5\n",
      "Epoch 50/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 0.9779 - val_loss: 0.9507\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.01807 to 0.95067, saving model to models/weights.best.hdf5\n",
      "Epoch 51/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 0.9055 - val_loss: 0.8677\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.95067 to 0.86772, saving model to models/weights.best.hdf5\n",
      "Epoch 52/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 0.8386 - val_loss: 0.8079\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.86772 to 0.80794, saving model to models/weights.best.hdf5\n",
      "Epoch 53/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 0.7818 - val_loss: 0.7746\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.80794 to 0.77456, saving model to models/weights.best.hdf5\n",
      "Epoch 54/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.7307 - val_loss: 0.7142\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.77456 to 0.71420, saving model to models/weights.best.hdf5\n",
      "Epoch 55/100\n",
      "9600/9600 [==============================] - 1s 78us/step - loss: 0.6869 - val_loss: 0.6715\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.71420 to 0.67147, saving model to models/weights.best.hdf5\n",
      "Epoch 56/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 0.6475 - val_loss: 0.6496\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.67147 to 0.64957, saving model to models/weights.best.hdf5\n",
      "Epoch 57/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 0.6079 - val_loss: 0.5906\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.64957 to 0.59056, saving model to models/weights.best.hdf5\n",
      "Epoch 58/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 0.5729 - val_loss: 0.5716\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.59056 to 0.57164, saving model to models/weights.best.hdf5\n",
      "Epoch 59/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.5427 - val_loss: 0.5721\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.57164\n",
      "Epoch 60/100\n",
      "9600/9600 [==============================] - 1s 78us/step - loss: 0.5127 - val_loss: 0.5024\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.57164 to 0.50240, saving model to models/weights.best.hdf5\n",
      "Epoch 61/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.4841 - val_loss: 0.4721\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.50240 to 0.47211, saving model to models/weights.best.hdf5\n",
      "Epoch 62/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 0.4592 - val_loss: 0.4486\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.47211 to 0.44856, saving model to models/weights.best.hdf5\n",
      "Epoch 63/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 0.4342 - val_loss: 0.4230\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.44856 to 0.42298, saving model to models/weights.best.hdf5\n",
      "Epoch 64/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 0.4084 - val_loss: 0.4027\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.42298 to 0.40274, saving model to models/weights.best.hdf5\n",
      "Epoch 65/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 0.3837 - val_loss: 0.3790\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.40274 to 0.37902, saving model to models/weights.best.hdf5\n",
      "Epoch 66/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 0.3637 - val_loss: 0.3618\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.37902 to 0.36177, saving model to models/weights.best.hdf5\n",
      "Epoch 67/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 0.3423 - val_loss: 0.3411\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.36177 to 0.34111, saving model to models/weights.best.hdf5\n",
      "Epoch 68/100\n",
      "9600/9600 [==============================] - 1s 78us/step - loss: 0.3226 - val_loss: 0.3211\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.34111 to 0.32105, saving model to models/weights.best.hdf5\n",
      "Epoch 69/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.3024 - val_loss: 0.2984\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.32105 to 0.29844, saving model to models/weights.best.hdf5\n",
      "Epoch 70/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 0.2855 - val_loss: 0.2802\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.29844 to 0.28016, saving model to models/weights.best.hdf5\n",
      "Epoch 71/100\n",
      "9600/9600 [==============================] - 1s 77us/step - loss: 0.2661 - val_loss: 0.2653\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.28016 to 0.26532, saving model to models/weights.best.hdf5\n",
      "Epoch 72/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 0.2494 - val_loss: 0.2481\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.26532 to 0.24806, saving model to models/weights.best.hdf5\n",
      "Epoch 73/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.2324 - val_loss: 0.2346\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.24806 to 0.23461, saving model to models/weights.best.hdf5\n",
      "Epoch 74/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 0.2155 - val_loss: 0.2207\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.23461 to 0.22066, saving model to models/weights.best.hdf5\n",
      "Epoch 75/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 0.2022 - val_loss: 0.2046\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.22066 to 0.20460, saving model to models/weights.best.hdf5\n",
      "Epoch 76/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 0.1878 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.20460 to 0.18690, saving model to models/weights.best.hdf5\n",
      "Epoch 77/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 0.1740 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.18690 to 0.18285, saving model to models/weights.best.hdf5\n",
      "Epoch 78/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 0.1637 - val_loss: 0.1681\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.18285 to 0.16811, saving model to models/weights.best.hdf5\n",
      "Epoch 79/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 0.1540 - val_loss: 0.1595\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.16811 to 0.15951, saving model to models/weights.best.hdf5\n",
      "Epoch 80/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 0.1418 - val_loss: 0.1493\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.15951 to 0.14931, saving model to models/weights.best.hdf5\n",
      "Epoch 81/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.1318 - val_loss: 0.1408\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.14931 to 0.14077, saving model to models/weights.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "9600/9600 [==============================] - 1s 71us/step - loss: 0.1215 - val_loss: 0.1267\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.14077 to 0.12665, saving model to models/weights.best.hdf5\n",
      "Epoch 83/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.1141 - val_loss: 0.1167\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.12665 to 0.11665, saving model to models/weights.best.hdf5\n",
      "Epoch 84/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 0.1062 - val_loss: 0.1189\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.11665\n",
      "Epoch 85/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.0979 - val_loss: 0.1072\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.11665 to 0.10721, saving model to models/weights.best.hdf5\n",
      "Epoch 86/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 0.0933 - val_loss: 0.0981\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.10721 to 0.09807, saving model to models/weights.best.hdf5\n",
      "Epoch 87/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 0.0887 - val_loss: 0.1001\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.09807\n",
      "Epoch 88/100\n",
      "9600/9600 [==============================] - 1s 78us/step - loss: 0.0809 - val_loss: 0.0891\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.09807 to 0.08910, saving model to models/weights.best.hdf5\n",
      "Epoch 89/100\n",
      "9600/9600 [==============================] - 1s 78us/step - loss: 0.0757 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.08910 to 0.08415, saving model to models/weights.best.hdf5\n",
      "Epoch 90/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.0758 - val_loss: 0.0862\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.08415\n",
      "Epoch 91/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 0.0706 - val_loss: 0.0783\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.08415 to 0.07832, saving model to models/weights.best.hdf5\n",
      "Epoch 92/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 0.0672 - val_loss: 0.0701\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.07832 to 0.07012, saving model to models/weights.best.hdf5\n",
      "Epoch 93/100\n",
      "9600/9600 [==============================] - 1s 78us/step - loss: 0.0630 - val_loss: 0.0619\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.07012 to 0.06193, saving model to models/weights.best.hdf5\n",
      "Epoch 94/100\n",
      "9600/9600 [==============================] - 1s 72us/step - loss: 0.0599 - val_loss: 0.0625\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.06193\n",
      "Epoch 95/100\n",
      "9600/9600 [==============================] - 1s 73us/step - loss: 0.0557 - val_loss: 0.0592\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.06193 to 0.05915, saving model to models/weights.best.hdf5\n",
      "Epoch 96/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 0.0522 - val_loss: 0.0575\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.05915 to 0.05751, saving model to models/weights.best.hdf5\n",
      "Epoch 97/100\n",
      "9600/9600 [==============================] - 1s 75us/step - loss: 0.0492 - val_loss: 0.0541\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.05751 to 0.05413, saving model to models/weights.best.hdf5\n",
      "Epoch 98/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.0477 - val_loss: 0.0515\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.05413 to 0.05150, saving model to models/weights.best.hdf5\n",
      "Epoch 99/100\n",
      "9600/9600 [==============================] - 1s 76us/step - loss: 0.0441 - val_loss: 0.0489\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.05150 to 0.04892, saving model to models/weights.best.hdf5\n",
      "Epoch 100/100\n",
      "9600/9600 [==============================] - 1s 74us/step - loss: 0.0433 - val_loss: 0.0492\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.04892\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "checkpointer = ModelCheckpoint(filepath='models/weights.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.0001))\n",
    "model.fit(X_train, Y_train, \n",
    "          validation_data=(X_val, Y_val),\n",
    "          shuffle=True, epochs=100, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "model.save('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "model.load_weights('models/weights.best.hdf5')\n",
    "model.save('models/model.best.h5')\n",
    "\n",
    "#model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = images_train\n",
    "Y_train = measurements_train\n",
    "\n",
    "X_val = images_val\n",
    "Y_val = measurements_val\n",
    "\n",
    "X_test = images_test\n",
    "Y_test = measurements_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping2d_9 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 90, 320, 3)        12        \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 86, 316, 10)       760       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 43, 158, 10)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 39, 154, 20)       5020      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 19, 77, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 15, 73, 30)        15030     \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_9 (Glob (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               15872     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 168,279\n",
      "Trainable params: 168,273\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, BatchNormalization, Dropout, AlphaDropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(160,320,3)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(10, (5,5), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(20, (5,5), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(30, (5,5), activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(512))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12348 samples, validate on 343 samples\n",
      "Epoch 1/20\n",
      "12348/12348 [==============================] - 12s 932us/step - loss: 0.1417 - val_loss: 0.0277\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02772, saving model to models/weights.best.hdf5\n",
      "Epoch 2/20\n",
      "12348/12348 [==============================] - 10s 845us/step - loss: 0.0915 - val_loss: 0.0364\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.02772\n",
      "Epoch 3/20\n",
      "12348/12348 [==============================] - 10s 844us/step - loss: 0.0644 - val_loss: 0.0367\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02772\n",
      "Epoch 4/20\n",
      "12348/12348 [==============================] - 10s 848us/step - loss: 0.0582 - val_loss: 0.0365\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02772\n",
      "Epoch 5/20\n",
      "12348/12348 [==============================] - 11s 851us/step - loss: 0.0534 - val_loss: 0.0296\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02772\n",
      "Epoch 6/20\n",
      "12348/12348 [==============================] - 10s 847us/step - loss: 0.0508 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02772\n",
      "Epoch 7/20\n",
      "12348/12348 [==============================] - 10s 832us/step - loss: 0.0472 - val_loss: 0.0340\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02772\n",
      "Epoch 8/20\n",
      "12348/12348 [==============================] - 10s 843us/step - loss: 0.0458 - val_loss: 0.0450\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02772\n",
      "Epoch 9/20\n",
      "12348/12348 [==============================] - 10s 842us/step - loss: 0.0443 - val_loss: 0.0310\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02772\n",
      "Epoch 10/20\n",
      "12348/12348 [==============================] - 11s 865us/step - loss: 0.0424 - val_loss: 0.0286\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02772\n",
      "Epoch 11/20\n",
      "12348/12348 [==============================] - 11s 880us/step - loss: 0.0405 - val_loss: 0.0388\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02772\n",
      "Epoch 12/20\n",
      "12348/12348 [==============================] - 11s 874us/step - loss: 0.0384 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02772\n",
      "Epoch 13/20\n",
      "12348/12348 [==============================] - 11s 868us/step - loss: 0.0362 - val_loss: 0.0316\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02772\n",
      "Epoch 14/20\n",
      "12348/12348 [==============================] - 11s 871us/step - loss: 0.0358 - val_loss: 0.0327\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02772\n",
      "Epoch 15/20\n",
      "12348/12348 [==============================] - 11s 879us/step - loss: 0.0333 - val_loss: 0.0318\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02772\n",
      "Epoch 16/20\n",
      "12348/12348 [==============================] - 11s 881us/step - loss: 0.0331 - val_loss: 0.0315\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02772\n",
      "Epoch 17/20\n",
      "12348/12348 [==============================] - 11s 880us/step - loss: 0.0312 - val_loss: 0.0379\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02772\n",
      "Epoch 18/20\n",
      "12348/12348 [==============================] - 11s 881us/step - loss: 0.0285 - val_loss: 0.0365\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02772\n",
      "Epoch 19/20\n",
      "12348/12348 [==============================] - 11s 877us/step - loss: 0.0276 - val_loss: 0.0337\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02772\n",
      "Epoch 20/20\n",
      "12348/12348 [==============================] - 11s 870us/step - loss: 0.0269 - val_loss: 0.0390\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02772\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "checkpointer = ModelCheckpoint(filepath='models/weights.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "model.fit(X_train, Y_train, \n",
    "          validation_data=(X_val, Y_val),\n",
    "          shuffle=True, epochs=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "model.save('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04487496146629023"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "#model.load_weights('models/weights.best.hdf5')\n",
    "model.save('models/model.best.h5')\n",
    "\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow3.6",
   "language": "python",
   "name": "tensorflow3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
